#!/usr/bin/env python3
"""
Qwen3-4B Training Script for UAR - Storage Optimized Version
This script only saves UAR-specific files to minimize storage usage.
"""

import os
import sys
import shutil
from llama_recipes.finetuning import main

def cleanup_large_model_files(output_dir):
    """
    æ¸…ç†å¤§å‹æ¨¡å‹æ–‡ä»¶ï¼Œåªä¿ç•™UARéœ€è¦çš„æ ¸å¿ƒæ–‡ä»¶
    """
    # UARå®é™…éœ€è¦çš„æ–‡ä»¶
    keep_files = {
        "config.json",           # æ¨¡å‹é…ç½®
        "tokenizer_config.json", # åˆ†è¯å™¨é…ç½®
        "tokenizer.json",        # åˆ†è¯å™¨æ–‡ä»¶
        "special_tokens_map.json", # ç‰¹æ®Štokenæ˜ å°„
        "filtered_model.pth"     # UARåˆ†ç±»å¤´æƒé‡ï¼ˆæ ¸å¿ƒæ–‡ä»¶ï¼‰
    }
    
    # éœ€è¦åˆ é™¤çš„å¤§å‹æ–‡ä»¶
    large_files_to_remove = [
        "pytorch_model.bin",
        "model.safetensors", 
        "model.safetensors.index.json",
        "model-00001-of-00002.safetensors",
        "model-00002-of-00002.safetensors",
        # å¯èƒ½çš„å…¶ä»–åˆ†ç‰‡æ–‡ä»¶
    ]
    
    removed_files = []
    saved_space = 0
    
    if not os.path.exists(output_dir):
        return removed_files, saved_space
    
    print(f"ğŸ§¹ æ¸…ç†ç›®å½•: {output_dir}")
    
    for filename in os.listdir(output_dir):
        file_path = os.path.join(output_dir, filename)
        
        # å¦‚æœæ˜¯å¤§å‹æ¨¡å‹æ–‡ä»¶ä¸”ä¸åœ¨ä¿ç•™åˆ—è¡¨ä¸­
        if filename in large_files_to_remove or (
            filename.startswith("model-") and filename.endswith(".safetensors")
        ):
            try:
                file_size = os.path.getsize(file_path)
                os.remove(file_path)
                removed_files.append(filename)
                saved_space += file_size
                print(f"  âœ“ åˆ é™¤: {filename} ({file_size / (1024**3):.2f}GB)")
            except Exception as e:
                print(f"  âœ— åˆ é™¤å¤±è´¥: {filename} - {e}")
    
    return removed_files, saved_space

def verify_uar_files(output_dir):
    """
    éªŒè¯UARæ ¸å¿ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    """
    required_files = [
        "config.json",
        "filtered_model.pth"  # UARåˆ†ç±»å¤´æƒé‡
    ]
    
    missing_files = []
    for file in required_files:
        if not os.path.exists(os.path.join(output_dir, file)):
            missing_files.append(file)
    
    return len(missing_files) == 0, missing_files

def train_qwen_classifiers_optimized(model_path="Qwen/Qwen2.5-4B", output_base_dir="./qwen_classifiers", 
                                   skip_completed=True, cleanup_after_training=True):
    """
    Train UAR classifiers with storage optimization
    """
    
    print("="*60)
    print("UAR Qwen3-4B åˆ†ç±»å™¨è®­ç»ƒç³»ç»Ÿ - å­˜å‚¨ä¼˜åŒ–ç‰ˆæœ¬")
    print("ç›®æ ‡ï¼šåªä¿å­˜UARæ ¸å¿ƒæ–‡ä»¶ï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´")
    print("="*60)
    
    # æ¨¡å‹è·¯å¾„æ£€æµ‹ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
    current_dir = os.getcwd()
    print(f"å½“å‰å·¥ä½œç›®å½•: {current_dir}")
    
    local_model_paths = [
        "/root/autodl-tmp/models/Qwen3-4B",
        "/root/autodl-tmp/models/Qwen2.5-4B",
        "/root/models/Qwen3-4B", 
        "/root/models/Qwen2.5-4B",
        "./models/Qwen3-4B",
        "./models/Qwen2.5-4B",
        "/root/autodl-tmp/UAR/models/Qwen3-4B",
        "/root/autodl-tmp/UAR/models/Qwen2.5-4B",
        model_path
    ]
    
    actual_model_path = None
    for path in local_model_paths:
        if os.path.exists(path) and os.path.isdir(path):
            model_files = ["config.json", "pytorch_model.bin", "model.safetensors"]
            has_model_files = any(os.path.exists(os.path.join(path, f)) for f in model_files)
            if has_model_files:
                actual_model_path = path
                print(f"âœ“ æ‰¾åˆ°æœ¬åœ°æ¨¡å‹: {path}")
                break
    
    if actual_model_path is None:
        print(f"âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹")
        return False
    
    model_path = actual_model_path
    os.makedirs(output_base_dir, exist_ok=True)
    
    # è®­ç»ƒå‚æ•°ï¼ˆå¤ç”¨åŸæœ‰é…ç½®ï¼‰
    common_args = {
        "model_name": model_path,
        "batch_size_training": 32,
        "val_batch_size": 32,
        "batching_strategy": "padding",
        "context_length": 4096,
        "gradient_accumulation_steps": 1,
        "lr": 5e-5,
        "weight_decay": 0.0,
        "gamma": 0.85,
        "num_epochs": 3,
        "num_workers_dataloader": 4,
        "reward_model_loss_type": "ce",
        "only_cls_for_rmce": True,  # UARæ ¸å¿ƒï¼šåªè®­ç»ƒåˆ†ç±»å¤´
        "use_fp16": True,
        "mixed_precision": True,
        "run_validation": True,
        "save_model": True,
        "quantization": False,
        "use_peft": False,
        "enable_fsdp": False,
        "one_gpu": True,
        "save_optimizer": False,
        "seed": 42,
    }
    
    # åˆ†ç±»å™¨é…ç½®
    classifiers = [
        {
            "name": "time_aware",
            "dataset": "time_aware_cls_ce",
            "output_dir": f"{output_base_dir}/Time_aware_qwen3_4b",
            "description": "Time Awareness Classifier"
        },
        {
            "name": "knowledge_aware", 
            "dataset": "knowledge_aware_cls_ce",
            "output_dir": f"{output_base_dir}/Know_aware_qwen3_4b",
            "description": "Knowledge Awareness Classifier"
        },
        {
            "name": "self_aware",
            "dataset": "self_aware_cls_ce_llama2_7b_chat",
            "output_dir": f"{output_base_dir}/Self_aware_qwen3_4b",
            "description": "Self Awareness Classifier"
        },
        {
            "name": "intent_aware",
            "dataset": "intent_aware_cls_ce", 
            "output_dir": f"{output_base_dir}/Intent_aware_qwen3_4b",
            "description": "Intent Awareness Classifier"
        }
    ]
    
    print(f"å­˜å‚¨ä¼˜åŒ–è®¾ç½®:")
    print(f"  - è®­ç»ƒåè‡ªåŠ¨æ¸…ç†: {cleanup_after_training}")
    print(f"  - åªä¿ç•™UARæ ¸å¿ƒæ–‡ä»¶: config.json, tokenizeræ–‡ä»¶, filtered_model.pth")
    print(f"  - åˆ é™¤å¤§å‹æ¨¡å‹æ–‡ä»¶: pytorch_model.bin, model.safetensorsç­‰")
    print("="*60)
    
    successful_classifiers = []
    failed_classifiers = []
    skipped_classifiers = []
    total_saved_space = 0
    
    for i, classifier in enumerate(classifiers, 1):
        output_dir = classifier['output_dir']
        
        # æ£€æŸ¥æ˜¯å¦å·²è®­ç»ƒå®Œæˆ
        if skip_completed:
            is_complete, missing = verify_uar_files(output_dir)
            if is_complete:
                print(f"\n[{i}/{len(classifiers)}] è·³è¿‡ {classifier['name']} - å·²è®­ç»ƒå®Œæˆ")
                successful_classifiers.append(classifier['name'])
                skipped_classifiers.append(classifier['name'])
                continue
        
        print(f"\n[{i}/{len(classifiers)}] è®­ç»ƒ {classifier['description']} ({classifier['name']})...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        train_args = {
            **common_args,
            "dataset": classifier["dataset"],
            "output_dir": output_dir
        }
        
        try:
            print(f"å¼€å§‹è®­ç»ƒ...")
            result = main(**train_args)
            print(f"âœ“ {classifier['name']} è®­ç»ƒå®Œæˆ")
            
            # è®­ç»ƒå®Œæˆåç«‹å³æ¸…ç†å¤§å‹æ–‡ä»¶
            if cleanup_after_training:
                print(f"ğŸ§¹ å¼€å§‹æ¸…ç†å­˜å‚¨...")
                removed_files, saved_space = cleanup_large_model_files(output_dir)
                total_saved_space += saved_space
                
                if removed_files:
                    print(f"âœ“ å·²åˆ é™¤ {len(removed_files)} ä¸ªå¤§å‹æ–‡ä»¶")
                    print(f"âœ“ èŠ‚çœç©ºé—´: {saved_space / (1024**3):.2f}GB")
                else:
                    print(f"â„¹ï¸  æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„å¤§å‹æ–‡ä»¶")
            
            # éªŒè¯UARæ–‡ä»¶å®Œæ•´æ€§
            is_complete, missing = verify_uar_files(output_dir)
            if is_complete:
                print(f"âœ“ UARæ ¸å¿ƒæ–‡ä»¶éªŒè¯é€šè¿‡")
                successful_classifiers.append(classifier['name'])
            else:
                print(f"âš ï¸  UARæ–‡ä»¶ä¸å®Œæ•´ï¼Œç¼ºå°‘: {missing}")
                
        except Exception as e:
            print(f"âœ— {classifier['name']} è®­ç»ƒå¤±è´¥: {e}")
            failed_classifiers.append(classifier['name'])
            continue
    
    # è®­ç»ƒå®Œæˆæ€»ç»“
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆæ€»ç»“")
    print("="*60)
    print(f"æˆåŠŸè®­ç»ƒ: {len(successful_classifiers)}/{len(classifiers)} ä¸ªåˆ†ç±»å™¨")
    
    if successful_classifiers:
        print(f"âœ“ æˆåŠŸ: {', '.join(successful_classifiers)}")
    
    if skipped_classifiers:
        print(f"â­ï¸  è·³è¿‡: {', '.join(skipped_classifiers)}")
        
    if failed_classifiers:
        print(f"âœ— å¤±è´¥: {', '.join(failed_classifiers)}")
    
    if total_saved_space > 0:
        print(f"ğŸ’¾ æ€»èŠ‚çœç©ºé—´: {total_saved_space / (1024**3):.2f}GB")
    
    print("="*60)
    
    return len(successful_classifiers) == len(classifiers)

def cleanup_existing_classifiers(base_dir="./qwen_classifiers"):
    """
    æ¸…ç†å·²æœ‰åˆ†ç±»å™¨ç›®å½•ä¸­çš„å¤§å‹æ–‡ä»¶
    """
    print("ğŸ§¹ æ¸…ç†ç°æœ‰åˆ†ç±»å™¨ç›®å½•...")
    
    if not os.path.exists(base_dir):
        print(f"ç›®å½•ä¸å­˜åœ¨: {base_dir}")
        return
    
    total_saved = 0
    cleaned_dirs = []
    
    for item in os.listdir(base_dir):
        item_path = os.path.join(base_dir, item)
        if os.path.isdir(item_path):
            # å…ˆéªŒè¯UARæ–‡ä»¶æ˜¯å¦å®Œæ•´
            is_complete, missing = verify_uar_files(item_path)
            if is_complete:
                removed_files, saved_space = cleanup_large_model_files(item_path)
                if saved_space > 0:
                    total_saved += saved_space
                    cleaned_dirs.append(item)
                    print(f"âœ“ {item}: èŠ‚çœ {saved_space / (1024**3):.2f}GB")
            else:
                print(f"âš ï¸  è·³è¿‡ {item}: UARæ–‡ä»¶ä¸å®Œæ•´ {missing}")
    
    if cleaned_dirs:
        print(f"\nğŸ‰ æ¸…ç†å®Œæˆ!")
        print(f"æ¸…ç†ç›®å½•: {len(cleaned_dirs)} ä¸ª")
        print(f"æ€»èŠ‚çœç©ºé—´: {total_saved / (1024**3):.2f}GB")
    else:
        print(f"â„¹ï¸  æœªæ‰¾åˆ°å¯æ¸…ç†çš„ç›®å½•")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="UAR Training with Storage Optimization")
    parser.add_argument("--model_path", type=str, default="Qwen/Qwen2.5-4B")
    parser.add_argument("--output_dir", type=str, default="./qwen_classifiers")
    parser.add_argument("--skip_completed", action="store_true", default=True)
    parser.add_argument("--cleanup", action="store_true", default=True,
                       help="Cleanup large model files after training")
    parser.add_argument("--cleanup_only", action="store_true", default=False,
                       help="Only cleanup existing directories, don't train")
    
    args = parser.parse_args()
    
    if args.cleanup_only:
        cleanup_existing_classifiers(args.output_dir)
    else:
        train_qwen_classifiers_optimized(
            model_path=args.model_path,
            output_base_dir=args.output_dir,
            skip_completed=args.skip_completed,
            cleanup_after_training=args.cleanup
        )
